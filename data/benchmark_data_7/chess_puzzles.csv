Model version,mean_score,Best score (across scorers),Release date,Organization,Country,Training compute (FLOP),Training compute notes,stderr,Log viewer,Logs,Started at,id
gemini-3-flash-preview,0.38,0.38,2025-12-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.04878317312145634,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FZMRExFd2WEEwTRSQPLXXwu.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZMRExFd2WEEwTRSQPLXXwu.eval,2025-12-17T22:40:30.291Z,ZMRExFd2WEEwTRSQPLXXwu
deepseek-reasoner,0.14,0.14,2025-12-01,DeepSeek,China,,,0.0348735088019777,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FCsCv6cEsjHtZnC6aoatVUC.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/CsCv6cEsjHtZnC6aoatVUC.eval,2025-12-16T14:33:28.567Z,CsCv6cEsjHtZnC6aoatVUC
gpt-5.2-2025-12-11_xhigh,0.49,0.49,2025-12-11,OpenAI,United States of America,,,0.05,,,2025-12-15T10:01:10.174Z,manual_gpt_5.2_xhigh_chess_puzzles_frankenstein
openai/gpt-oss-120b_high,0.2,0.2,2025-08-05,OpenAI,United States of America,4.94e+24,"""The training run for gpt-oss-120b required 2.1 million H100-hours to complete""
(2.1e6 hours)*(1,979 H100 FLOP/s)*(30% utilization)*(60*60) = 4.49e24
They also do post training similar to o3, which we assume adds at least 10% as much compute, so we multiply this estimate by 1.1 to get 4.94e24

",0.030977345909486177,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fh529FgFpbSYUfLa8s3CocQ.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/h529FgFpbSYUfLa8s3CocQ.eval,2025-12-11T22:37:40.881Z,h529FgFpbSYUfLa8s3CocQ
gpt-5.2-2025-12-11_medium,0.4,0.4,2025-12-11,OpenAI,United States of America,,,0.0492365963917331,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FfCzTfk9GNPokGEVSg2ry4s.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/fCzTfk9GNPokGEVSg2ry4s.eval,2025-12-11T20:47:46.894Z,fCzTfk9GNPokGEVSg2ry4s
gpt-5.2-2025-12-11_high,0.4,0.4,2025-12-11,OpenAI,United States of America,,,0.0492365963917331,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FR3bUSJCQiuGjg6TzBo7k8Q.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/R3bUSJCQiuGjg6TzBo7k8Q.eval,2025-12-11T20:47:46.679Z,R3bUSJCQiuGjg6TzBo7k8Q
gpt-5.2-2025-12-11_low,0.23,0.23,2025-12-11,OpenAI,United States of America,,,0.04229525846816508,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FSUBFSiYKfaantxswKig2a2.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/SUBFSiYKfaantxswKig2a2.eval,2025-12-11T20:24:27.647Z,SUBFSiYKfaantxswKig2a2
Qwen3-235B-A22B-Thinking-2507,0.12244897959183673,0.12244897959183673,2025-07-25,Alibaba,China,4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,0.03328341697069118,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F7iH4kjDYrjgRwQdKYgNACT.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/7iH4kjDYrjgRwQdKYgNACT.eval,2025-12-11T15:00:30.667Z,7iH4kjDYrjgRwQdKYgNACT
qwen3-max-2025-09-23,0.04,0.04,2025-09-24,Alibaba,China,1.512e+25,"6ND with:
36T tokens is taken from the qwen3 technical report
70B active params is based on it having >1T params, and the architectures of Qwen3-235B-A22B and Qwen3-Coder-480B-A35B",0.01969463855669324,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FWfe5udNStR4yt4eoqgZ8hY.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Wfe5udNStR4yt4eoqgZ8hY.eval,2025-12-10T16:04:15.360Z,Wfe5udNStR4yt4eoqgZ8hY
kimi-k2-thinking-turbo,0.2,0.2,2025-11-06,Moonshot,China,4.2e+24,"Assuming the additional post-training contributed between 1% and 100% of Kimi K2's training compute (which we confidently estimate at 2.976e+24), we get a range of 3.0e24 to 6.0e24, and a geometric mean of 4.2e24.",0.04020151261036849,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F8JwFyEbMELziSdLLg8hatX.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/8JwFyEbMELziSdLLg8hatX.eval,2025-12-10T16:03:31.928Z,8JwFyEbMELziSdLLg8hatX
claude-sonnet-4-5-20250929_32K,0.12,0.12,2025-09-29,Anthropic,United States of America,,,0.032659863237109045,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F6qgxJVk8HHLppb7hPqz9Tv.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/6qgxJVk8HHLppb7hPqz9Tv.eval,2025-12-08T14:58:44.624Z,6qgxJVk8HHLppb7hPqz9Tv
o4-mini-2025-04-16_high,0.26,0.26,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.04408440022768081,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fk2G5DrB2We92UEZnGoyVuL.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/k2G5DrB2We92UEZnGoyVuL.eval,2025-12-08T14:58:44.100Z,k2G5DrB2We92UEZnGoyVuL
gpt-5.1-2025-11-13_high,0.32,0.32,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",0.04688261722621506,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F8aknqG8nWu3P5CoBcetqte.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/8aknqG8nWu3P5CoBcetqte.eval,2025-12-08T14:58:42.139Z,8aknqG8nWu3P5CoBcetqte
gpt-5-2025-08-07_high,0.37,0.37,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.048523658709390974,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F6DcmBdRZz57U5cusZNBeGW.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/6DcmBdRZz57U5cusZNBeGW.eval,2025-12-08T14:58:41.672Z,6DcmBdRZz57U5cusZNBeGW
o3-mini-2025-01-31_high,0.17,0.17,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.03775251680686369,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F52bwKmNetKXPAGYaBjdgCB.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/52bwKmNetKXPAGYaBjdgCB.eval,2025-12-08T14:58:41.327Z,52bwKmNetKXPAGYaBjdgCB
claude-opus-4-5-20251101_32K,0.12,0.12,2025-11-24,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.032659863237109045,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FP7kyrA6oQ68tVGHyekP7Qj.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/P7kyrA6oQ68tVGHyekP7Qj.eval,2025-12-08T14:58:41.000Z,P7kyrA6oQ68tVGHyekP7Qj
grok-4-0709,0.35064935064935066,0.35064935064935066,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.05473553444308601,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F6igkYQndntWtiswYh8SfyR.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/6igkYQndntWtiswYh8SfyR.eval,2025-12-08T14:58:40.610Z,6igkYQndntWtiswYh8SfyR
gemini-2.5-pro,0.2,0.2,2025-06-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.04020151261036849,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FmQ3UwD6hUioWSp2s2oELY4.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/mQ3UwD6hUioWSp2s2oELY4.eval,2025-12-08T14:58:40.497Z,mQ3UwD6hUioWSp2s2oELY4
gemini-3-pro-preview,0.31,0.31,2025-11-18,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Almost certainly >1e25,0.04648231987117317,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FEy65gb5LG3RdtZxMweDfAU.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Ey65gb5LG3RdtZxMweDfAU.eval,2025-12-08T14:58:39.203Z,Ey65gb5LG3RdtZxMweDfAU
