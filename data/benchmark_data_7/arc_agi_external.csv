Model version,Score,Release date,Organization,Country,Training compute (FLOP),Training compute notes,Name,Source,Source link,Notes,id
gpt-5.2-2025-12-11_xhigh,0.862,2025-12-11,OpenAI,United States of America,,,GPT-5.2 (X-High),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recgxHD7jV9nk6VyZ
claude-opus-4-5-20251101_64K,0.8,2025-11-24,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Opus 4.5 (64k thinking),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recXBzrtTETaQuvxw
gpt-5.2-2025-12-11_high,0.787,2025-12-11,OpenAI,United States of America,,,GPT-5.2 (High),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recG0lrSvCvamnnBm
claude-opus-4-5-20251101_32K,0.758,2025-11-24,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Opus 4.5 (32k thinking),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recn5WFdr0nWTOt7b
,0.757,,,,,,o3-preview (Low)*,ARC Prize Leaderboard,https://arcprize.org/leaderboard,Excluded since there will never be any other benchmark data for it apart from FrontierMath,recM25loZNyA1teVD
gemini-3-pro-preview,0.75,2025-11-18,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Almost certainly >1e25,Gemini 3 Pro,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recJT3M7RL0TgeABi
gpt-5.1-2025-11-13_high,0.728,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",GPT-5.1 (High),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec8sLsUfujiSrlk4
gpt-5-pro-2025-10-06_high,0.702,2025-10-07,OpenAI,United States of America,,,GPT-5 Pro,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec4jlS7emdBeDgkx
gpt-5-2025-08-07_high,0.657,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",GPT-5 (High),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recy0dyn1Ex4jNC4H
o3-2025-04-16_high,0.608,2025-04-16,OpenAI,United States of America,,,o3 (High),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recyX2f3amNA4ZPUQ
o3-pro-2025-06-10_high,0.593,2025-06-10,OpenAI,United States of America,,,o3-Pro (High),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recgEreMuMqYTWJuX
o4-mini-2025-04-16_high,0.587,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",o4-mini (High),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,receup1qngSep4blO
gpt-5.1-2025-11-13_medium,0.577,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",GPT-5.1 (Medium),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,reccC3Lgebztl4MAm
o3-pro-2025-06-10_medium,0.57,2025-06-10,OpenAI,United States of America,,,o3-Pro (Medium),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,reco8WbS3E6S6DSwq
gpt-5-2025-08-07_medium,0.562,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",GPT-5 (Medium),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recfuBzPqyWo5BEjp
o3-2025-04-16_medium,0.538,2025-04-16,OpenAI,United States of America,,,o3 (Medium),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,reczsOK58YJa6GAAz
o3-pro-2025-06-10_low,0.443,2025-06-10,OpenAI,United States of America,,,o3-Pro (Low),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec2DTU08MgXm0hds
gpt-5-2025-08-07_low,0.44,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",GPT-5 (Low),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recyhP0MFpu4yP9Fs
o4-mini-2025-04-16_medium,0.418,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",o4-mini (Medium),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recFnX3IAcQd2NffG
o3-2025-04-16_low,0.415,2025-04-16,OpenAI,United States of America,,,o3 (Low),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recmynZQA4oogWrzw
claude-sonnet-4-20250514_16K,0.4,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Sonnet 4 (Thinking 16K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec00IX4WEZ6XvDQd
claude-opus-4-20250514_16K,0.357,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Opus 4 (Thinking 16K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recpPgRxRuNQ6MJbU
o3-mini-2025-01-31_high,0.345,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",o3-mini (High),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recsslT51vOvwD7Hw
gemini-2.5-flash-preview-05-20_16K,0.333,2025-05-20,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,Gemini 2.5 Flash (Preview) (Thinking 16K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recPvvngATSJ9tzRl
gemini-2.5-flash-preview-05-20,0.333,2025-05-20,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,Gemini 2.5 Flash (Preview),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recRaEsoFzvHSY2ds
,0.331,,,,,,GPT-5.1 (Low),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rectUG8eOKZjJ1xGs
gemini-2.5-pro-preview-03-25,0.33,2025-04-09,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Gemini 2.5 Pro (Preview),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recGhSAAXTywTt6ks
gemini-2.5-flash-preview-04-17 (24K thinking),0.323,2025-04-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,Gemini 2.5 Flash (Preview) (Thinking 24K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec9VDig3HkReY0oP
gemini-2.5-pro-preview-06-05_1K,0.313,2025-06-05,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,"Gemini 2.5 Pro (Preview, Thinking 1K)",ARC Prize Leaderboard,https://arcprize.org/leaderboard,Thinking budgets have only been supported since 06-05,recdEFBjW5RECOz15
o1-2024-12-17_medium,0.307,2024-12-17,OpenAI,United States of America,,,o1 (Medium),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec3bVHAcLI4a7u7z
claude-opus-4-20250514_8K,0.307,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Opus 4 (Thinking 8K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec5eQ6jRbh3n8fQc
claude-sonnet-4-20250514_8K,0.29,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Sonnet 4 (Thinking 8K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recd23dJHC8IgD2IC
claude-3-7-sonnet-20250219_16K,0.286,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,Claude 3.7 (16K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recgR391dmSWyZjKu
claude-sonnet-4-20250514_1K,0.28,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Sonnet 4 (Thinking 1K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recs4iQhoJgUrLWrd
codex-mini-2025-05-16,0.273,2025-05-16,,,,,Codex Mini (Latest),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recBLorgitaes8FMx
o1-2024-12-17_low,0.272,2024-12-17,OpenAI,United States of America,,,o1 (Low),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recxLLEulVCEVpIcG
claude-opus-4-20250514_1K,0.27,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Opus 4 (Thinking 1K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recOlznt9pRz5uq9J
gemini-2.5-flash-preview-05-20_8K,0.258,2025-05-20,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,Gemini 2.5 Flash (Preview) (Thinking 8K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recy1lONU8pakDYKr
claude-sonnet-4-20250514,0.238,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Sonnet 4,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recXVePhVjOhYoNSC
o1-pro-2025-03-19_low,0.233,2025-03-19,OpenAI,United States of America,,,o1-pro (Low),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recW7rLYsNpIjAlPE
claude-opus-4-20250514,0.225,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Claude Opus 4,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recU2yBBdPh7FDErC
o3-mini-2025-01-31_medium,0.223,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",o3-mini (Medium),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recKkBnHhDmmhkCSp
o4-mini-2025-04-16_low,0.213,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",o4-mini (Low),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,reci1Fl7wFPBUAxIi
claude-3-7-sonnet-20250219_8K,0.212,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,Claude 3.7 (8K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,reckQgsdCUEH1BmYZ
DeepSeek-R1-0528,0.212,2025-05-28,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",Deepseek R1 (05/28),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recsscdqM4qvGHGT3
o1-preview-2024-09-12,0.18,2024-09-12,OpenAI,United States of America,,,o1-preview,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,reclttZQlaXCYpTDb
grok-3-mini-beta_low,0.165,2025-04-09,xAI,United States of America,,,Grok 3 Mini (Low),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recUL8yDJO8uotk8q
gemini-2.5-flash-preview-05-20_1K,0.16,2025-05-20,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,Gemini 2.5 Flash (Preview) (Thinking 1K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recxGd7FrJeq7ZyUy
DeepSeek-R1,0.158,2025-01-20,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",Deepseek R1,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec9Em7ivae72BNBO
o3-mini-2025-01-31_low,0.145,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",o3-mini (Low),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recoKTXJX6Wtyzmyn
o1-mini-2024-09-12_medium,0.14,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",o1-mini,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recjBMo8w3uRVYaQN
claude-3-7-sonnet-20250219,0.136,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,Claude 3.7,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recDy7yrZGrBGUAKD
claude-3-7-sonnet-20250219_1K,0.116,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,Claude 3.7 (1K),ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec96fPyndWwzLTzI
gpt-4.5-preview-2025-02-27,0.103,2025-02-27,OpenAI,United States of America,3.8e+26,"Analysis of GPT-4.5's training cluster yields a median estimate of 187M H100-hours of training. The utilization assumptions we used for the Grok 3 estimate (probably worth revisiting) were 20 to 40% under the H100 FP8 spec of 2000 teraflop/s. This leads to an estimate of 2.7e26 to 5.4e26 FLOP, or a geomean of 3.8e26

Alternatively, using a plausible range of 20 to 50% utilization, given the possibility of FP8 training, yields a median estimate of ~2e25 FLOP. See notebook below for details.

https://colab.research.google.com/drive/1QBmVPm64Ti0xucN0EsZTgSz_I7Mj9hAZ#scrollTo=NYH1ABJuLJlw 

This is consistent with OpenAI's statement that GPT-4.5 was a “new order of magnitude in compute” compared to previous models (e.g. GPT-4, which was ~2e25), suggesting around 2e26 FLOP. But they could have meant this somewhat loosely. 

In the ""Pretraining GPT-4.5"" interview, they state they used multi-cluster training: https://youtu.be/6nJZopACRuQ?si=FFJC-gEmGPZjvoPM&t=617 ",GPT-4.5,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recqWXrYnfvx1C2KT
gpt-4.1-2025-04-14,0.055,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,GPT-4.1,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec3x5nkdnWzgTzeC
grok-3,0.055,2025-04-09,xAI,United States of America,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",Grok 3,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec6qqzTzHa8ARTi6
gpt-4o-2024-11-20,0.045,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,GPT-4o,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recEtb28sN0f3OFf5
Llama-4-Maverick-17B-128E-Instruct,0.044,2025-04-05,Meta AI,United States of America,2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",Llama 4 Maverick,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,rec0EiReUxsja9Iui
gpt-4.1-mini-2025-04-14,0.035,2025-04-14,OpenAI,United States of America,,,GPT-4.1-Mini,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recFVGAte0Uy8kc22
Llama-4-Scout-17B-16E-Instruct,0.005,2025-04-05,Meta AI,United States of America,4.08e+24,"40T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  

Estimating training compute from parameters and tokens:
6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP
(Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)


The model card also states that Llama 4 Scout used 5.0M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)",Llama 4 Scout,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recNN11X9GLUAxRYd
gpt-4.1-nano-2025-04-14,0.0,2025-04-14,OpenAI,United States of America,,,GPT-4.1-Nano,ARC Prize Leaderboard,https://arcprize.org/leaderboard,,recNs9X6RPBPUfcRN
