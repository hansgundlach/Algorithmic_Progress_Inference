Model version,mean_score,Best score (across scorers),Release date,Organization,Country,Training compute (FLOP),Training compute notes,stderr,Log viewer,Logs,Started at,id
fireworks/deepseek-v3p2,0.221,0.221,2025-12-01,DeepSeek,China,,,0.041,,,2025-12-22T16:13:27.305Z,manual_deepseek-v3.2_frontiermath_tiers_13_private_frankenstein_run_incomplete
gemini-2.5-flash,0.04844290657439446,0.04844290657439446,2025-06-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.012651332738454618,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/bTbDNLFqsUogG3fejZEAZc.eval,2025-12-18T18:23:15.700Z,bTbDNLFqsUogG3fejZEAZc
gemini-3-flash-preview,0.356401384083045,0.356401384083045,2025-12-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.028221558784583784,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/R7dSNsq2ciVRVHB7nir69v.eval,2025-12-17T22:40:29.699Z,R7dSNsq2ciVRVHB7nir69v
gpt-5.2-2025-12-11_xhigh,0.407,0.407,2025-12-11,OpenAI,United States of America,,,0.029,,,2025-12-13T13:45:00.000Z,hYCViU82i8tejydKA4Pcuu
gpt-5.2-2025-12-11_high,0.40344827586206894,0.40344827586206894,2025-12-11,OpenAI,United States of America,,,0.028858188319787523,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/dcQtgoUnQbkqTT6Cueo8Re.eval,2025-12-11T20:49:17.862Z,dcQtgoUnQbkqTT6Cueo8Re
gpt-5.2-2025-12-11_medium,0.3689655172413793,0.3689655172413793,2025-12-11,OpenAI,United States of America,,,0.02838379845926905,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/BWGVnFerZCyKTuc86w7c5q.eval,2025-12-11T20:49:17.514Z,BWGVnFerZCyKTuc86w7c5q
gpt-5.2-2025-12-11_low,0.2655172413793103,0.2655172413793103,2025-12-11,OpenAI,United States of America,,,0.02597695517914575,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/NghuUppaqcCUtT8BqRgUL6.eval,2025-12-11T20:24:40.964Z,NghuUppaqcCUtT8BqRgUL6
Qwen/Qwen3-235B-A22B-Thinking-2507,0.08480565371024736,0.08480565371024736,2025-07-25,Alibaba,China,4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,0.016589928458951776,,https://epoch-benchmarks-staging-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/TLsGMRt4tYtWnheLm6jA5s.eval,2025-12-11T17:22:27.962Z,TLsGMRt4tYtWnheLm6jA5s
zai-org/GLM-4.6,0.03819444444444445,0.03819444444444445,2025-09-30,"Z.ai (Zhipu AI),Tsinghua University",China,4.42e+24,6 FLOP/parameter/token * 32000000000 active parameters [very likely assumption - everything else is reported to be same as at GLM 4.5] * 23000000000000 tokens = 4.42e24 FLOP,0.011313651347924142,,https://epoch-benchmarks-staging-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/MPJBHaxt8mXcKBWa94rsXj.eval,2025-12-08T20:08:02.803Z,MPJBHaxt8mXcKBWa94rsXj
moonshotai/Kimi-K2-Thinking,0.21403508771929824,0.21403508771929824,2025-11-06,Moonshot,China,4.2e+24,"Assuming the additional post-training contributed between 1% and 100% of Kimi K2's training compute (which we confidently estimate at 2.976e+24), we get a range of 3.0e24 to 6.0e24, and a geometric mean of 4.2e24.",0.02433800055326239,,https://epoch-benchmarks-staging-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/FmXuaTot9QsNdFYwwFHuEG.eval,2025-12-05T16:22:32.448Z,FmXuaTot9QsNdFYwwFHuEG
gpt-5.1-2025-11-13_none,0.02097902097902098,0.02097902097902098,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",0.008489188512823553,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/XNbh4FSUJeFAe2auBg5Bdi.eval,2025-11-25T22:22:28.656Z,XNbh4FSUJeFAe2auBg5Bdi
gpt-5.1-2025-11-13_low,0.17301038062283736,0.17301038062283736,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",0.022288963139763994,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/7o6HCQaKUrxWjrf2vhH7hV.eval,2025-11-25T15:12:29.299Z,7o6HCQaKUrxWjrf2vhH7hV
claude-opus-4-5-20251101,0.20689655172413793,0.20689655172413793,2025-11-24,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02382827611454509,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/9nctB7cH6kcLT24UbScEMa.eval,2025-11-25T04:02:38.737Z,9nctB7cH6kcLT24UbScEMa
claude-opus-4-5-20251101_32K,0.20689655172413793,0.20689655172413793,2025-11-24,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02382827611454509,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/DVjCqcKsfQ4kJbwucCiuH7.eval,2025-11-25T01:11:04.094Z,DVjCqcKsfQ4kJbwucCiuH7
claude-opus-4-5-20251101_16K,0.20344827586206896,0.20344827586206896,2025-11-24,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.023680184210831092,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Xvp9LtJuwPtSab63ZWwkTC.eval,2025-11-25T01:10:48.342Z,Xvp9LtJuwPtSab63ZWwkTC
gemini-2.5-pro,0.1413793103448276,0.1413793103448276,2025-06-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.020494847099131453,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/cA9LKKKzDVxiLRcegKSb2b.eval,2025-11-24T02:37:34.534Z,cA9LKKKzDVxiLRcegKSb2b
gemini-3-pro-preview,0.376,0.376,2025-11-18,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Almost certainly >1e25,0.0284,,,2025-11-21T12:00:00.000Z,manual_gemini_3_fmprivate_frankenstein
gpt-5.1-2025-11-13_medium,0.2689655172413793,0.2689655172413793,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",0.026083646905766342,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/7c2hHEgF9JomG9X7BLP7By.eval,2025-11-17T21:28:57.104Z,7c2hHEgF9JomG9X7BLP7By
o3-2025-04-16_low,0.09722222222222222,0.09722222222222222,2025-04-16,OpenAI,United States of America,,,0.017487682248157596,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/DKLrszPeTvFUGq39AaTniX.eval,2025-11-17T01:17:15.370Z,DKLrszPeTvFUGq39AaTniX
o3-2025-04-16_high,0.18685121107266436,0.18685121107266436,2025-04-16,OpenAI,United States of America,,,0.022968716344899227,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/XQ46BgPhHaYZsrPQuTvz8J.eval,2025-11-16T20:35:07.675Z,XQ46BgPhHaYZsrPQuTvz8J
o3-2025-04-16_medium,0.16896551724137931,0.16896551724137931,2025-04-16,OpenAI,United States of America,,,0.022042438716799215,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/XwNqEvxaaCQ8M2NpKJGoZG.eval,2025-11-16T20:35:03.993Z,XwNqEvxaaCQ8M2NpKJGoZG
claude-sonnet-4-5-20250929,0.09310344827586207,0.09310344827586207,2025-09-29,Anthropic,United States of America,,,0.017092785280147835,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/n26C8XHigC9Xfsdb24JZtd.eval,2025-11-16T12:36:38.474Z,n26C8XHigC9Xfsdb24JZtd
claude-sonnet-4-5-20250929_32K,0.1522491349480969,0.1522491349480969,2025-09-29,Anthropic,United States of America,,,0.021169728769293714,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/LUpSd7DzWf3Fw4iwXb3MPV.eval,2025-11-16T12:36:27.931Z,LUpSd7DzWf3Fw4iwXb3MPV
o4-mini-2025-04-16_low,0.10689655172413794,0.10689655172413794,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.018175392977158443,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/82dL9ukqav96KZ8aHfqPpq.eval,2025-11-16T03:21:42.143Z,82dL9ukqav96KZ8aHfqPpq
o3-mini-2025-01-31_high,0.12413793103448276,0.12413793103448276,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.019396402576684655,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/W2YTmg94XqVbH7p86TAqJT.eval,2025-11-16T01:50:26.218Z,W2YTmg94XqVbH7p86TAqJT
gpt-5.1-2025-11-13_high,0.3103448275862069,0.3103448275862069,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",0.02721380905679257,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/nveMCdbvDijDTBN9JPWNve.eval,2025-11-13T20:36:07.951Z,nveMCdbvDijDTBN9JPWNve
gpt-5-2025-08-07_high,0.32413793103448274,0.32413793103448274,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.027532461664839192,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/JwfCZuni4TRRnA77UWwpyg.eval,2025-11-13T19:38:13.991Z,JwfCZuni4TRRnA77UWwpyg
claude-sonnet-4-5-20250929_59K,0.13494809688581316,0.13494809688581316,2025-09-29,Anthropic,United States of America,,,0.020132987375676092,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/3GzYKQW2K4iijm6a73rVeh.eval,2025-11-13T15:54:58.691Z,3GzYKQW2K4iijm6a73rVeh
gpt-5-mini-2025-08-07_high,0.27241379310344827,0.27241379310344827,2025-08-07,OpenAI,United States of America,,,0.02618833296520288,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/DkMXbLvTgWJDWnUQVuBA8J.eval,2025-11-13T15:54:56.299Z,DkMXbLvTgWJDWnUQVuBA8J
o4-mini-2025-04-16_medium,0.1896551724137931,0.1896551724137931,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.023060480732052885,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/XPCVZtFEC6BYKAwhLauMSD.eval,2025-11-13T15:54:51.979Z,XPCVZtFEC6BYKAwhLauMSD
o4-mini-2025-04-16_high,0.2482758620689655,0.2482758620689655,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.02541251077219606,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Z7hiWJnEkkLmQXviEiTdHt.eval,2025-11-13T15:54:49.087Z,Z7hiWJnEkkLmQXviEiTdHt
gpt-5-mini-2025-08-07_medium,0.20344827586206896,0.20344827586206896,2025-08-07,OpenAI,United States of America,,,0.023680184210831092,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/VPHGtT9EdPS6YxbzDAt98q.eval,2025-11-13T15:54:46.227Z,VPHGtT9EdPS6YxbzDAt98q
grok-4-0709,0.19655172413793104,0.19655172413793104,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.023375906908472147,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/oN54TKXxcn9gofYBASTVCY.eval,2025-11-13T15:54:39.607Z,oN54TKXxcn9gofYBASTVCY
gpt-5-2025-08-07_medium,0.27241379310344827,0.27241379310344827,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.02618833296520288,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/hrJBSLQUe5hrHECkuySSJr.eval,2025-11-13T15:54:39.070Z,hrJBSLQUe5hrHECkuySSJr
gpt-5-nano-2025-08-07_high,0.08275862068965517,0.08275862068965517,2025-08-07,OpenAI,United States of America,,,0.01620688385836769,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/2eMHpXXR9h7P2oNPDQkTBY.eval,2025-10-30T16:20:13.144Z,2eMHpXXR9h7P2oNPDQkTBY
claude-haiku-4-5-20251001_32K,0.059027777777777776,0.059027777777777776,2025-10-15,Anthropic,United States of America,,,0.013911554772795832,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/fdNzD5mp8r2bHxy2PXx6Gh.eval,2025-10-22T12:29:30.662Z,fdNzD5mp8r2bHxy2PXx6Gh
claude-haiku-4-5-20251001,0.041379310344827586,0.041379310344827586,2025-10-15,Anthropic,United States of America,,,0.011715642254112493,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/8sTJmYZ7MSk8obK9vjUjHm.eval,2025-10-16T09:34:31.339Z,8sTJmYZ7MSk8obK9vjUjHm
gpt-5-nano-2025-08-07_medium,0.07241379310344828,0.07241379310344828,2025-08-07,OpenAI,United States of America,,,0.015245401561146893,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/dHQzrNGjuR5kiHKtNxG62X.eval,2025-08-07T18:43:21.321Z,dHQzrNGjuR5kiHKtNxG62X
claude-opus-4-20250514_27K,0.041379310344827586,0.041379310344827586,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.011715642254112493,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZKroUnGN6qoNssoSQbtzzz.eval,2025-08-05T23:13:11.048Z,ZKroUnGN6qoNssoSQbtzzz
claude-opus-4-1-20250805_27K,0.07241379310344828,0.07241379310344828,2025-08-05,Anthropic,United States of America,,,0.015245401561146893,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ezJ5C38svWJkQMjxTcrgg7.eval,2025-08-05T19:56:41.845Z,ezJ5C38svWJkQMjxTcrgg7
claude-opus-4-1-20250805,0.05862068965517241,0.05862068965517241,2025-08-05,Anthropic,United States of America,,,0.013818435156390344,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/TQQnqmugDk3GX5hkr8HMWz.eval,2025-08-05T19:56:40.259Z,TQQnqmugDk3GX5hkr8HMWz
claude-opus-4-20250514,0.04482758620689655,0.04482758620689655,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.012172075609474642,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/GKD2ELzaXHx3giARevEetr.eval,2025-07-04T10:42:31.483Z,GKD2ELzaXHx3giARevEetr
claude-sonnet-4-20250514,0.041379310344827586,0.041379310344827586,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.011715642254112493,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/UcfxxJmo4ftRzPEnXrRdSw.eval,2025-07-04T10:41:31.264Z,UcfxxJmo4ftRzPEnXrRdSw
gemini-2.5-pro-preview-06-05,0.10344827586206896,0.10344827586206896,2025-06-05,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.01791432224407271,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/hNLQNC6TASXNNzat8kVS77.eval,2025-07-03T13:57:21.637Z,hNLQNC6TASXNNzat8kVS77
qwen-plus-2025-04-28,0.017241379310344827,0.017241379310344827,2025-04-28,Alibaba,China,,,0.007657032895812118,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/bVFEkuHiUSxjoh8SQm74jo.eval,2025-05-14T13:10:51.812Z,bVFEkuHiUSxjoh8SQm74jo
mistral-medium-2505,0.0034602076124567475,0.0034602076124567475,2025-05-07,Mistral AI,France,,"Benchmarks match with models like GPT-4o, Mistral's previous largest runs were ~1e25 FLOP scale, so plausibly they might have trained Medium with this much compute.",0.003460207612456747,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/PZRX6ME25bn26B6BTRWx6W.eval,2025-05-07T17:33:33.886Z,PZRX6ME25bn26B6BTRWx6W
gpt-4.1-nano-2025-04-14,0.010344827586206896,0.010344827586206896,2025-04-14,OpenAI,United States of America,,,0.0059518867144507945,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/chtQvebcoaE33KkhAjbCnw.eval,2025-04-14T22:51:44.342Z,chtQvebcoaE33KkhAjbCnw
gpt-4.1-mini-2025-04-14,0.04482758620689655,0.04482758620689655,2025-04-14,OpenAI,United States of America,,,0.01217207560947464,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ephMMUGDANc9iVTNNuuoZx.eval,2025-04-14T22:49:44.216Z,ephMMUGDANc9iVTNNuuoZx
gpt-4.1-2025-04-14,0.05517241379310345,0.05517241379310345,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.013430381628597847,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/NPeiYpjbscreYnFkHPvcNB.eval,2025-04-14T22:40:01.283Z,NPeiYpjbscreYnFkHPvcNB
grok-3-mini-beta_low,0.027586206896551724,0.027586206896551724,2025-04-09,xAI,United States of America,,,0.009634354634513523,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/9FTMS2t6Ksm43CduHeqtJr.eval,2025-04-10T20:30:21.697Z,9FTMS2t6Ksm43CduHeqtJr
grok-3-mini-beta_high,0.05862068965517241,0.05862068965517241,2025-04-09,xAI,United States of America,,,0.013818435156390344,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Z83F6bJ7Ne5rXaXwgR2xWT.eval,2025-04-10T18:02:17.992Z,Z83F6bJ7Ne5rXaXwgR2xWT
grok-3-beta,0.03793103448275862,0.03793103448275862,2025-04-09,xAI,United States of America,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",0.011237029601999628,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/DzjFQArKFTZHioQVwKYVtX.eval,2025-04-10T17:57:03.446Z,DzjFQArKFTZHioQVwKYVtX
Llama-4-Maverick-17B-128E-Instruct-FP8,0.006896551724137931,0.006896551724137931,2025-04-05,Meta AI,United States of America,2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",0.004868154158215009,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/QjvBS34fsngfBBMZUirSyh.eval,2025-04-08T10:17:16.175Z,QjvBS34fsngfBBMZUirSyh
Llama-4-Scout-17B-16E-Instruct,0.0,0.0,2025-04-05,Meta AI,United States of America,4.08e+24,"40T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  

Estimating training compute from parameters and tokens:
6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP
(Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)


The model card also states that Llama 4 Scout used 5.0M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)",0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/bhjLDA5A7gddsX5cAYKma8.eval,2025-04-08T10:14:56.486Z,bhjLDA5A7gddsX5cAYKma8
qwen-max-2025-01-25,0.010344827586206896,0.010344827586206896,2025-01-25,Alibaba,China,,,0.005951886714450794,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/F77vDPynTKJbz7m2bt7Py8.eval,2025-04-02T16:55:37.430Z,F77vDPynTKJbz7m2bt7Py8
claude-3-7-sonnet-20250219_64K,0.03103448275862069,0.03103448275862069,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.01020064175308735,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/DoU54Xy93J3MxztC4uxwYy.eval,2025-03-13T16:49:59.754Z,DoU54Xy93J3MxztC4uxwYy
claude-3-7-sonnet-20250219_32K,0.034482758620689655,0.034482758620689655,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.010733271038801586,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/aZqVpPa4SWHapN9QHEoEm3.eval,2025-03-13T09:34:11.446Z,aZqVpPa4SWHapN9QHEoEm3
gemini-2.0-flash-001,0.017241379310344827,0.017241379310344827,2025-02-05,"Google DeepMind,Google","United Kingdom of Great Britain and Northern Ireland,United States of America",,"""We used Trillium TPUs to train the new Gemini 2.0, Google’s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",0.007657032895812118,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/5RpSr8gJZfcZn586aV2hDW.eval,2025-03-09T17:18:49.858Z,5RpSr8gJZfcZn586aV2hDW
DeepSeek-V3,0.017241379310344827,0.017241379310344827,2024-12-26,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",0.007657032895812118,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/aXcfCytWFYNk7E5AZGMzLb.eval,2025-03-07T23:36:27.507Z,aXcfCytWFYNk7E5AZGMzLb
o1-2024-12-17_high,0.09310344827586207,0.09310344827586207,2024-12-17,OpenAI,United States of America,,,0.017092785280147835,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/bynpttv6jLvbXTY7ktnBpk.eval,2025-03-07T19:46:02.827Z,bynpttv6jLvbXTY7ktnBpk
gpt-4o-2024-08-06,0.0034482758620689655,0.0034482758620689655,2024-08-06,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.003448275862068965,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/SmT4ukjkxsXZvFExMMVFgW.eval,2025-03-07T18:56:32.370Z,SmT4ukjkxsXZvFExMMVFgW
claude-3-5-sonnet-20240620,0.010344827586206896,0.010344827586206896,2024-06-20,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.0059518867144507945,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/h7f9jZVWzZNDHNxP5eewNH.eval,2025-03-07T18:54:11.499Z,h7f9jZVWzZNDHNxP5eewNH
gemini-1.5-flash-002,0.0,0.0,2024-09-24,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/8FtfWCm8DdAqF9rnMVwv8h.eval,2025-03-07T18:40:43.945Z,8FtfWCm8DdAqF9rnMVwv8h
claude-3-5-haiku-20241022,0.0034482758620689655,0.0034482758620689655,2024-10-22,Anthropic,United States of America,,,0.003448275862068965,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/JQcqpW9xPfJRKJkHne6DpK.eval,2025-03-07T18:11:31.647Z,JQcqpW9xPfJRKJkHne6DpK
claude-3-5-sonnet-20241022,0.020689655172413793,0.020689655172413793,2024-10-22,Anthropic,United States of America,,,0.008373130807525473,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/MVnDrt94wyokzx3a7ZJkph.eval,2025-03-06T23:54:46.118Z,MVnDrt94wyokzx3a7ZJkph
o1-mini-2024-09-12_medium,0.017241379310344827,0.017241379310344827,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.007657032895812118,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/hC2FY4WuKm6BLgPhbWeZDY.eval,2025-03-06T22:32:06.577Z,hC2FY4WuKm6BLgPhbWeZDY
o1-mini-2024-09-12_high,0.013793103448275862,0.013793103448275862,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.006860663093423027,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/2ekT2UzaebvmGprKvmeeEa.eval,2025-03-06T22:30:16.233Z,2ekT2UzaebvmGprKvmeeEa
claude-3-7-sonnet-20250219_16K,0.041379310344827586,0.041379310344827586,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.011715642254112493,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/PU7LXbdNB8Unfvu4qCeJZF.eval,2025-03-06T19:16:31.497Z,PU7LXbdNB8Unfvu4qCeJZF
grok-2-1212,0.006896551724137931,0.006896551724137931,2024-12-12,xAI,United States of America,2.96e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,0.004868154158215009,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/kinyvFjqSqUiY5FyYv7tsA.eval,2025-03-06T18:06:22.963Z,kinyvFjqSqUiY5FyYv7tsA
o3-mini-2025-01-31_medium,0.08081896551724138,0.08081896551724138,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.012761417139125066,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/U8CjtcxudQeP2cnMNixfJH.eval,2025-03-06T18:06:08.042Z,U8CjtcxudQeP2cnMNixfJH
claude-3-7-sonnet-20250219,0.03103448275862069,0.03103448275862069,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.010200641753087351,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/m7X2ppkiEkweCtFppiRW8S.eval,2025-03-06T18:05:58.538Z,m7X2ppkiEkweCtFppiRW8S
mistral-large-2411,0.0034482758620689655,0.0034482758620689655,2024-11-18,Mistral AI,France,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",0.003448275862068965,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/cEvTUVyxAqq5UDgsGQbvLM.eval,2025-03-06T18:05:57.178Z,cEvTUVyxAqq5UDgsGQbvLM
gpt-4o-2024-11-20,0.0034482758620689655,0.0034482758620689655,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.003448275862068965,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Z7HU95T8eema56EpLtECvG.eval,2025-03-06T18:03:58.731Z,Z7HU95T8eema56EpLtECvG
gemini-2.5-deep-think-2025-08-01-webapp,0.29,0.29,2025-08-01,"Google,Google DeepMind","United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.027,,,,manual_gemini_2.5_deep_think_run_frontiermath_tiers_1_to_3
