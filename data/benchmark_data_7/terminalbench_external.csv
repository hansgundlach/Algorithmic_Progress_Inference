Model version,Agent,Accuracy mean,Release date,Organization,Country,Training compute (FLOP),Training compute notes,Accuracy SE,Agent Org,Model Org,Run date,Notes,Source,Source Link,Created,id
gpt-5.2-2025-12-11_medium,Droid,0.649,2025-12-11,OpenAI,United States of America,,,0.028,Factory,OpenAI,2025-12-24,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2026-01-06T08:12:25.000Z,rechjHEUQLlqCTIXc
gemini-3-flash-preview,Junie CLI,0.643,2025-12-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.028,JetBrains,Google,2025-12-23,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2026-01-06T08:13:09.000Z,recUBHBNuIOfkcKRX
claude-opus-4-5-20251101,Droid,0.631,2025-11-24,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.027,Factory,Anthropic,2025-12-11,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-12-15T17:58:37.000Z,recuiEXbA7NpkFohL
gpt-5.2-2025-12-11_medium,Codex CLI,0.629,2025-12-11,OpenAI,United States of America,,,0.03,OpenAI,OpenAI,2025-12-18,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2026-01-06T08:14:05.000Z,recumDtosjSEJ2D3d
gemini-3-pro-preview,II-Agent,0.618,2025-11-18,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Almost certainly >1e25,0.028,Intelligent Internet,Google,2025-12-23,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2026-01-06T08:14:47.000Z,rectX4lsyYs6OmUsy
,Warp,0.612,,,,,,0.03,Warp,Multiple,2025-12-12,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-12-15T18:01:01.000Z,recfdpeJOkmpBb3Fx
gemini-3-pro-preview,Droid,0.611,2025-11-18,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Almost certainly >1e25,0.028,Factory,Google,2025-12-24,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2026-01-06T08:15:23.000Z,recNP2GjuvuiMcEyv
gpt-5.1-codex-max,Codex CLI,0.604,2025-11-19,OpenAI,United States of America,,,0.027,OpenAI,OpenAI,2025-11-24,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-12-15T18:02:58.000Z,recaaVycAGesmFJ32
claude-opus-4-5-20251101_128K,Letta Code,0.591,2025-11-24,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.024,Letta,Anthropic,2025-12-17,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-25T18:37:51.000Z,reciL106EieXnavcX
gemini-3-pro-preview,II-Agent,0.589,2025-11-18,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Almost certainly >1e25,0.029,Intelligent Internet,Google,2025-11-24,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-12-17T19:47:25.000Z,rec5pj4ecLTmGF5xm
gpt-5.1-codex,Codex CLI,0.578,2025-11-12,OpenAI,United States of America,,,0.029, OpenAI      , OpenAI      ,2025-11-16,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:10:29.000Z,recninlGInIspn658
gemini-3-pro-preview,Terminus 2,0.542,2025-11-18,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Almost certainly >1e25,0.024,Stanford,Google,2025-11-18,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:19:25.000Z,rec4G1Mf13x9ynwlw
gpt-5.1-codex,Letta Code,0.535,2025-11-12,OpenAI,United States of America,,,0.028,Letta,OpenAI,2025-12-17,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2026-01-06T08:25:23.000Z,recdETGOmZWS2iKAe
gemini-3-flash-preview, Gemini CLI     ,0.51,2025-12-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.03,Google,Google,2025-12-23,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2026-01-06T08:23:54.000Z,recXnaoWnwYiDCTLY
gpt-5-2025-08-07_medium, Codex CLI      ,0.496,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.029, OpenAI    , OpenAI      ,2025-11-04,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec6jIJxArc92pivy
gpt-5.1-2025-11-13_medium,Terminus 2,0.476,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",0.028,Stanford,OpenAI,2025-11-16,Assuming medium based on GPT-5 being run at medium.,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:10:29.000Z,rec8JKbUbg1BqQUuc
gpt-5.1-2025-11-13_medium, Terminus 2     ,0.476,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",0.028, Stanford  , OpenAI      ,2025-11-16,Assuming medium based on GPT-5 being run at medium.,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recTEUNVY8Ta5gHeC
gpt-5-codex, Codex CLI      ,0.443,2025-09-15,OpenAI,United States of America,,"OpenAI says this is a version of GPT-5. The meaning of this is not completely clear, but likely means a shared base model and a similar/overlapping post-training history. But it is not clear whether 5-codex was produced via additional fine-tuning of a published GPT-5 checkpoint.",0.027, OpenAI    , OpenAI      ,2025-11-04,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec1qBXfk7ZcFVViF
gpt-5-2025-08-07_medium, OpenHands      ,0.438,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.03, OpenHands , OpenAI      ,2025-11-02,"Medium thinking used in other GPT-5 evaluations, assuming the same here.",Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recimrZ9t1kWdq9df
gpt-5-codex, Terminus 2     ,0.434,2025-09-15,OpenAI,United States of America,,"OpenAI says this is a version of GPT-5. The meaning of this is not completely clear, but likely means a shared base model and a similar/overlapping post-training history. But it is not clear whether 5-codex was produced via additional fine-tuning of a published GPT-5 checkpoint.",0.029, Stanford  , OpenAI      ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recXSpZpHeeVjiI3u
gpt-5.1-codex-mini, Codex CLI      ,0.431,2025-11-12,OpenAI,United States of America,,,0.03, OpenAI    , OpenAI      ,2025-11-17,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recJBKBBdOIEodbRT
claude-sonnet-4-5-20250929, Terminus 2     ,0.428,2025-09-29,Anthropic,United States of America,,,0.028, Stanford  , Anthropic   ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recZH7p8iYiyYB0Ch
claude-sonnet-4-5-20250929, OpenHands      ,0.426,2025-09-29,Anthropic,United States of America,,,0.028, OpenHands , Anthropic   ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,reclDcpO9SUv4Tpau
claude-sonnet-4-5-20250929, Mini-SWE-Agent ,0.425,2025-09-29,Anthropic,United States of America,,,0.028, Princeton , Anthropic   ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recfCRv3Mp3k41IHO
gpt-5-codex, Mini-SWE-Agent ,0.413,2025-09-15,OpenAI,United States of America,,"OpenAI says this is a version of GPT-5. The meaning of this is not completely clear, but likely means a shared base model and a similar/overlapping post-training history. But it is not clear whether 5-codex was produced via additional fine-tuning of a published GPT-5 checkpoint.",0.028, Princeton , OpenAI      ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec9sfNDeDk0cbJQL
claude-sonnet-4-5-20250929, Claude Code    ,0.401,2025-09-29,Anthropic,United States of America,,,0.029, Anthropic , Anthropic   ,2025-11-04,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recDripAbQH8CuooL
claude-opus-4-1-20250805, Terminus 2     ,0.38,2025-08-05,Anthropic,United States of America,,,0.026, Stanford  , Anthropic   ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recZqrNbY83D2rMna
gpt-5.1-codex, Terminus 2     ,0.369,2025-11-12,OpenAI,United States of America,,,0.032, Stanford  , OpenAI      ,2025-11-17,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec65Jj1nz0W6LZFC
claude-opus-4-1-20250805, OpenHands      ,0.369,2025-08-05,Anthropic,United States of America,,,0.027, OpenHands , Anthropic   ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recO3hSKFRHSXO0v0
kimi-k2-thinking, Terminus 2     ,0.357,2025-11-06,Moonshot,China,4.2e+24,"Assuming the additional post-training contributed between 1% and 100% of Kimi K2's training compute (which we confidently estimate at 2.976e+24), we get a range of 3.0e24 to 6.0e24, and a geometric mean of 4.2e24.",0.028, Stanford  , Moonshot AI ,2025-11-11,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec2gS7bw3o4JruIn
gpt-5-2025-08-07_medium, Terminus 2     ,0.352,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.031, Stanford  , OpenAI      ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recbqMxsZI87hk2Zv
claude-opus-4-1-20250805, Mini-SWE-Agent ,0.351,2025-08-05,Anthropic,United States of America,,,0.025, Princeton , Anthropic   ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec1kbUd5yf48B3vK
claude-opus-4-1-20250805, Claude Code    ,0.348,2025-08-05,Anthropic,United States of America,,,0.029, Anthropic , Anthropic   ,2025-11-04,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recIh4lviUhDM9aux
gpt-5-2025-08-07_medium, Mini-SWE-Agent ,0.339,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.029, Princeton , OpenAI      ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recN22yvosEoDYoao
gemini-2.5-pro, Terminus 2     ,0.326,2025-06-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.03, Stanford  , Google      ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rect0YAzqrcZe6qxi
gpt-5-mini-2025-08-07_medium, Codex CLI      ,0.319,2025-08-07,OpenAI,United States of America,,,0.03, OpenAI    , OpenAI      ,2025-11-04,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recNTuh2AD5nZEqvj
, Terminus 2     ,0.3,,,,,,0.027, Stanford  , MiniMax     ,2025-11-01,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recbhNOUse5RLyy5B
claude-haiku-4-5-20251001, Mini-SWE-Agent ,0.298,2025-10-15,Anthropic,United States of America,,,0.025, Princeton , Anthropic   ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recjYJxgaZXKRW8nE
gpt-5-mini-2025-08-07_medium, OpenHands      ,0.292,2025-08-07,OpenAI,United States of America,,,0.028, OpenHands , OpenAI      ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recqpHHQIxrdzWQ2e
claude-haiku-4-5-20251001, Terminus 2     ,0.283,2025-10-15,Anthropic,United States of America,,,0.029, Stanford  , Anthropic   ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recBFoPPbvHLqQkIF
Kimi-K2-Instruct, Terminus 2     ,0.278,2025-07-12,Moonshot,China,2.976e+24,6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP,0.025, Stanford  , Moonshot AI ,2025-11-01,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recxOKDiLuVBjbuwV
claude-haiku-4-5-20251001, Claude Code    ,0.275,2025-10-15,Anthropic,United States of America,,,0.028, Anthropic , Anthropic   ,2025-11-04,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,reca6RNNkwrVNSBgQ
grok-4-0709, OpenHands      ,0.272,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.031, OpenHands , xAI         ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recSY6n1UG1WaG3BJ
Kimi-K2-Instruct, OpenHands      ,0.267,2025-07-12,Moonshot,China,2.976e+24,6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP,0.027, OpenHands , Moonshot AI ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recZ35cLEBZqtObMV
gemini-2.5-pro, Mini-SWE-Agent ,0.261,2025-06-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.025, Princeton , Google      ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,reczBJ8FZoe7Ihdp1
grok-code-fast-1, Mini-SWE-Agent ,0.258,2025-08-28,,,,,0.026, Princeton , xAI         ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recJFHV7PmdXOIqt5
grok-4-0709, Mini-SWE-Agent ,0.254,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.029, Princeton , xAI         ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec2SbVuabwABv4Ag
Qwen3-Coder-480B-A35B-Instruct, OpenHands      ,0.254,2025-07-31,Alibaba,China,1.575e+24,6 FLOP / parameter / token * 35 * 10^9 active parameters * 7.5 * 10^12 tokens = 1.575e+24 FLOP`,0.026, OpenHands , Alibaba     ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec6ayRIToMb9XXDx
glm-4.6, Terminus 2     ,0.245,2025-09-30,"Z.ai (Zhipu AI),Tsinghua University",China,4.42e+24,6 FLOP/parameter/token * 32000000000 active parameters [very likely assumption - everything else is reported to be same as at GLM 4.5] * 23000000000000 tokens = 4.42e24 FLOP,0.024, Stanford  , Z.ai        ,2025-11-01,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recHvj6HO3PYp6H5Z
gpt-5-mini-2025-08-07_medium, Terminus 2     ,0.24,2025-08-07,OpenAI,United States of America,,,0.025, Stanford  , OpenAI      ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recmHdcWhKi0CzznQ
Qwen3-Coder-480B-A35B-Instruct, Terminus 2     ,0.239,2025-07-31,Alibaba,China,1.575e+24,6 FLOP / parameter / token * 35 * 10^9 active parameters * 7.5 * 10^12 tokens = 1.575e+24 FLOP`,0.028, Stanford  , Alibaba     ,2025-11-01,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recjc6bHW1H4vwXQE
grok-4-0709, Terminus 2     ,0.231,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.029, Stanford  , xAI         ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec8wkritwB2vOG3E
gpt-5-mini-2025-08-07_medium, Mini-SWE-Agent ,0.222,2025-08-07,OpenAI,United States of America,,,0.026, Princeton , OpenAI      ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recnD7LCzSjD2bMOB
gemini-2.5-pro, Gemini CLI     ,0.196,2025-06-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.029, Google    , Google      ,2025-11-04,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec263O7vZj3zvozA
gpt-oss-120b, Terminus 2     ,0.187,2025-08-05,OpenAI,United States of America,4.94e+24,"""The training run for gpt-oss-120b required 2.1 million H100-hours to complete""
(2.1e6 hours)*(1,979 H100 FLOP/s)*(30% utilization)*(60*60) = 4.49e24
They also do post training similar to o3, which we assume adds at least 10% as much compute, so we multiply this estimate by 1.1 to get 4.94e24

",0.027, Stanford  , OpenAI      ,2025-11-01,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recQOII1RUaWlRx1a
gemini-2.5-flash-preview-09-2025, Mini-SWE-Agent ,0.171,2025-09-25,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.025, Princeton , Google      ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec5CS4eSqb3DAWFd
gemini-2.5-flash-preview-09-2025, Terminus 2     ,0.169,2025-09-25,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.024, Stanford  , Google      ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recPFqIRxpwoxFWNM
gemini-2.5-pro, OpenHands      ,0.164,2025-06-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.028, OpenHands , Google      ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recHlyAnoUE2kdSdW
gemini-2.5-flash-preview-09-2025, OpenHands      ,0.164,2025-09-25,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.024, OpenHands , Google      ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recqa4Q4kJzmyqp3p
gemini-2.5-flash-preview-09-2025, Gemini CLI     ,0.154,2025-09-25,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.023, Google    , Google      ,2025-11-04,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recr9kiUX6cDggDYx
grok-code-fast-1, Terminus 2     ,0.142,2025-08-28,,,,,0.025, Stanford  , xAI         ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rech8rVcJ0mZR84p6
gpt-oss-120b, Mini-SWE-Agent ,0.142,2025-08-05,OpenAI,United States of America,4.94e+24,"""The training run for gpt-oss-120b required 2.1 million H100-hours to complete""
(2.1e6 hours)*(1,979 H100 FLOP/s)*(30% utilization)*(60*60) = 4.49e24
They also do post training similar to o3, which we assume adds at least 10% as much compute, so we multiply this estimate by 1.1 to get 4.94e24

",0.023, Princeton , OpenAI      ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,reczhxfX0o9hQLBlA
claude-haiku-4-5-20251001, OpenHands      ,0.139,2025-10-15,Anthropic,United States of America,,,0.027, OpenHands , Anthropic   ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec2PAXFlaUXgbf7W
gpt-5-nano-2025-08-07_medium, Codex CLI      ,0.115,2025-08-07,OpenAI,United States of America,,,0.023, OpenAI    , OpenAI      ,2025-11-04,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recPmu1Q5fQdfQ0Qg
gpt-5-nano-2025-08-07_medium, OpenHands      ,0.099,2025-08-07,OpenAI,United States of America,,,0.021, OpenHands , OpenAI      ,2025-11-02,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,reckUrafyTwxe61sP
gpt-5-nano-2025-08-07_medium, Terminus 2     ,0.079,2025-08-07,OpenAI,United States of America,,,0.019, Stanford  , OpenAI      ,2025-10-31,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec1PtzzU1cAULZYv
gpt-5-nano-2025-08-07_medium, Mini-SWE-Agent ,0.07,2025-08-07,OpenAI,United States of America,,,0.019, Princeton , OpenAI      ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,rec9SLPT2R9rByofO
gpt-oss-20b, Mini-SWE-Agent ,0.034,2025-08-05,OpenAI,United States of America,5.49e+23,"""The training run for gpt-oss-120b required 2.1
million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer""
assuming ""almost 10x fewer"" means ~9x fewer: 4.94e24/9 = 5.49e23",0.014, Princeton , OpenAI      ,2025-11-03,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recXJetcV0C5gLRl3
gpt-oss-20b, Terminus 2     ,0.031,2025-08-05,OpenAI,United States of America,5.49e+23,"""The training run for gpt-oss-120b required 2.1
million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer""
assuming ""almost 10x fewer"" means ~9x fewer: 4.94e24/9 = 5.49e23",0.015, Stanford  , OpenAI      ,2025-11-01,,Terminal-Bench v2 Leaderboard,https://www.tbench.ai/leaderboard/terminal-bench/2.0,2025-11-18T19:32:42.000Z,recaaiHI5eYtINDFb
