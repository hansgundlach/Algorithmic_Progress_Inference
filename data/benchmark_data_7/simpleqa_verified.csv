Model version,mean_score,Best score (across scorers),Release date,Organization,Country,Training compute (FLOP),Training compute notes,stderr,Log viewer,Logs,Started at,id
gemini-3-flash-preview,0.674,0.674,2025-12-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,,0.014830507204541049,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FU8cZYV5sPCiqQBVdx3vT7D.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/U8cZYV5sPCiqQBVdx3vT7D.eval,2025-12-17T22:40:31.128Z,U8cZYV5sPCiqQBVdx3vT7D
deepseek-reasoner,0.275,0.275,2025-12-01,DeepSeek,China,,,0.0141270865564905,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMB78rXsEm7srHraZ94mfP8.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MB78rXsEm7srHraZ94mfP8.eval,2025-12-16T17:15:27.799Z,MB78rXsEm7srHraZ94mfP8
openai/gpt-oss-120b_high,0.139,0.139,2025-08-05,OpenAI,United States of America,4.94e+24,"""The training run for gpt-oss-120b required 2.1 million H100-hours to complete""
(2.1e6 hours)*(1,979 H100 FLOP/s)*(30% utilization)*(60*60) = 4.49e24
They also do post training similar to o3, which we assume adds at least 10% as much compute, so we multiply this estimate by 1.1 to get 4.94e24

",0.010945263761042892,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMAVCYoXJFEuBWUsJpSgnLL.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MAVCYoXJFEuBWUsJpSgnLL.eval,2025-12-15T19:40:01.804Z,MAVCYoXJFEuBWUsJpSgnLL
Qwen3-235B-A22B-Thinking-2507,0.501002004008016,0.501002004008016,2025-07-25,Alibaba,China,4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,0.01583512708307639,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FS9wG8ps9RJGx49yx4G4ucG.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/S9wG8ps9RJGx49yx4G4ucG.eval,2025-12-11T23:03:54.696Z,S9wG8ps9RJGx49yx4G4ucG
gpt-5.2-2025-12-11_medium,0.354,0.354,2025-12-11,OpenAI,United States of America,,,0.015129868238451812,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FVviSGU9aryKLdpxWdaNh64.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/VviSGU9aryKLdpxWdaNh64.eval,2025-12-11T20:49:22.107Z,VviSGU9aryKLdpxWdaNh64
gpt-5.2-2025-12-11_high,0.382,0.382,2025-12-11,OpenAI,United States of America,,,0.015372453034968502,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fauo6Q7RrtQhAHVvhpEYEa9.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/auo6Q7RrtQhAHVvhpEYEa9.eval,2025-12-11T20:47:42.769Z,auo6Q7RrtQhAHVvhpEYEa9
gpt-5.2-2025-12-11_xhigh,0.389,0.389,2025-12-11,OpenAI,United States of America,,,0.01542455564730851,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FUYhgwCjfRSMMPG4hCSRa2G.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/UYhgwCjfRSMMPG4hCSRa2G.eval,2025-12-11T20:45:04.130Z,UYhgwCjfRSMMPG4hCSRa2G
gpt-5.2-2025-12-11_low,0.347,0.347,2025-12-11,OpenAI,United States of America,,,0.015060472031706637,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FkMbHY56qnwvVEWgRdsRz76.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/kMbHY56qnwvVEWgRdsRz76.eval,2025-12-11T20:24:19.578Z,kMbHY56qnwvVEWgRdsRz76
kimi-k2-thinking-turbo,0.316,0.316,2025-11-06,Moonshot,China,4.2e+24,"Assuming the additional post-training contributed between 1% and 100% of Kimi K2's training compute (which we confidently estimate at 2.976e+24), we get a range of 3.0e24 to 6.0e24, and a geometric mean of 4.2e24.",0.014709193056057165,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FjKZHTbZ5U8q5eaHhcRpa2c.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/jKZHTbZ5U8q5eaHhcRpa2c.eval,2025-12-10T16:03:29.803Z,jKZHTbZ5U8q5eaHhcRpa2c
qwen3-max-2025-09-23,0.6746746746746747,0.6746746746746747,2025-09-24,Alibaba,China,1.512e+25,"6ND with:
36T tokens is taken from the qwen3 technical report
70B active params is based on it having >1T params, and the architectures of Qwen3-235B-A22B and Qwen3-Coder-480B-A35B",0.014829990399788268,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FNqovLMDwVNAgEx6eBF9UVv.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/NqovLMDwVNAgEx6eBF9UVv.eval,2025-12-10T11:27:42.516Z,NqovLMDwVNAgEx6eBF9UVv
claude-sonnet-4-5-20250929_59K,0.236,0.236,2025-09-29,Anthropic,United States of America,,,0.013434451402438602,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FdkaSsY6ekRwpM3vtazggz9.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/dkaSsY6ekRwpM3vtazggz9.eval,2025-12-09T11:24:16.281Z,dkaSsY6ekRwpM3vtazggz9
gpt-5.1-2025-11-13_high,0.489,0.489,2025-11-13,OpenAI,United States of America,,"Almost certainly > 1e25
",0.015815471195292575,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fdh7LNDccM6B42QLmtEPqdP.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/dh7LNDccM6B42QLmtEPqdP.eval,2025-12-09T11:24:13.771Z,dh7LNDccM6B42QLmtEPqdP
claude-opus-4-1-20250805_27K,0.348,0.348,2025-08-05,Anthropic,United States of America,,,0.015070604603768327,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FLZCCEmpg43qgBA7oA3gHJY.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/LZCCEmpg43qgBA7oA3gHJY.eval,2025-12-09T11:24:13.371Z,LZCCEmpg43qgBA7oA3gHJY
gpt-5-mini-2025-08-07_high,0.21,0.21,2025-08-07,OpenAI,United States of America,,,0.012886662332274642,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F6rApyrxeqn3mqgHLwvjJLi.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/6rApyrxeqn3mqgHLwvjJLi.eval,2025-12-09T11:24:13.008Z,6rApyrxeqn3mqgHLwvjJLi
claude-haiku-4-5-20251001_32K,0.059,0.059,2025-10-15,Anthropic,United States of America,,,0.007454835650406693,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FSTBWLpBSME2mUt8vJYDzZX.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/STBWLpBSME2mUt8vJYDzZX.eval,2025-12-09T11:24:12.978Z,STBWLpBSME2mUt8vJYDzZX
o4-mini-2025-04-16_high,0.239,0.239,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.013493000446937702,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F8WntV54Uoar4tWGr3mZzAw.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/8WntV54Uoar4tWGr3mZzAw.eval,2025-12-09T11:24:12.699Z,8WntV54Uoar4tWGr3mZzAw
claude-opus-4-5-20251101_32K,0.418,0.418,2025-11-24,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.015605111967541902,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FHLpuoWwjoSAoDN4ucrtwgZ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/HLpuoWwjoSAoDN4ucrtwgZ.eval,2025-12-09T11:24:11.855Z,HLpuoWwjoSAoDN4ucrtwgZ
o3-2025-04-16_high,0.53,0.53,2025-04-16,OpenAI,United States of America,,,0.015790799515836722,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FGQemdo58TwkgXMG2PMzWXi.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/GQemdo58TwkgXMG2PMzWXi.eval,2025-12-09T11:24:11.270Z,GQemdo58TwkgXMG2PMzWXi
gpt-5-nano-2025-08-07_high,0.122,0.122,2025-08-07,OpenAI,United States of America,,,0.010354864712936776,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FKw4sds5ZsQxMUxzjMv27kC.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Kw4sds5ZsQxMUxzjMv27kC.eval,2025-12-09T11:24:09.741Z,Kw4sds5ZsQxMUxzjMv27kC
grok-3-mini-beta_high,0.211,0.211,2025-04-09,xAI,United States of America,,,0.012909130321041991,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FQC6wy3hW9ZxKeB3d49C8fr.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/QC6wy3hW9ZxKeB3d49C8fr.eval,2025-12-09T11:24:07.481Z,QC6wy3hW9ZxKeB3d49C8fr
gpt-5-2025-08-07_high,0.506,0.506,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.015818160898606833,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FaHMMDbUHESBXUUHNpgTP8f.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/aHMMDbUHESBXUUHNpgTP8f.eval,2025-12-09T11:24:04.595Z,aHMMDbUHESBXUUHNpgTP8f
grok-4-0709,0.479,0.479,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.015805341148131185,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FNWHWcdTydfvjogLRfSo9wr.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/NWHWcdTydfvjogLRfSo9wr.eval,2025-12-08T19:30:53.749Z,NWHWcdTydfvjogLRfSo9wr
claude-sonnet-4-5-20250929,0.13,0.13,2025-09-29,Anthropic,United States of America,,,0.010640169792499236,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FJN5Az748JWC33A4DjyDBrF.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/JN5Az748JWC33A4DjyDBrF.eval,2025-12-08T19:30:53.217Z,JN5Az748JWC33A4DjyDBrF
gemini-2.5-pro,0.56,0.56,2025-06-17,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.015704987954361718,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fk3SiTGeJf9ATxkLsVJ4KYZ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/k3SiTGeJf9ATxkLsVJ4KYZ.eval,2025-12-08T19:30:51.962Z,k3SiTGeJf9ATxkLsVJ4KYZ
DeepSeek-R1-0528,0.274,0.274,2025-05-28,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",0.014111099288259692,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FKbt46CuiJ3BJVjQMCpccZK.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Kbt46CuiJ3BJVjQMCpccZK.eval,2025-12-08T19:30:51.367Z,Kbt46CuiJ3BJVjQMCpccZK
gemini-3-pro-preview,0.729,0.729,2025-11-18,Google DeepMind,"United Kingdom of Great Britain and Northern Ireland,United States of America",,Almost certainly >1e25,0.014062601350986123,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F6uqK6uGAmMee8pkuX2oz5Q.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/6uqK6uGAmMee8pkuX2oz5Q.eval,2025-12-08T19:30:49.486Z,6uqK6uGAmMee8pkuX2oz5Q
claude-3-5-haiku-20241022,0.067,0.067,2024-10-22,Anthropic,United States of America,,,0.007910345983177575,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FTtvi2yQBmZ6L7jV86d9dH6.eval,https://epoch-benchmarks-staging-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Ttvi2yQBmZ6L7jV86d9dH6.eval,2025-11-28T16:32:06.613Z,Ttvi2yQBmZ6L7jV86d9dH6
